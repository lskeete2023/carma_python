{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrieval I\n",
    "\n",
    "In this notebook, we will work with the following:\n",
    "\n",
    "- Web scraping process.\n",
    "- Read one page.\n",
    "- Find the content we want.\n",
    "- Automate many pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUESTS GETS URL\n",
    "# beatutiful soup parses web pages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"mode.copy_on_write\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "One helpful way of gathering text data is web scraping.\n",
    "We usually do this in three steps:\n",
    "\n",
    "1. Retrieve the pages with information we want.\n",
    "1. Extract the data from the pages.\n",
    "1. Clean and save the resulting data.\n",
    "\n",
    "Let's walk through an example of getting press releases from the [Microsoft website](https://news.microsoft.com/category/press-releases/).\n",
    "\n",
    "I often prefer to work out of order as follows:\n",
    "\n",
    "1. Figure out how to extract data from one page that has the data.\n",
    "1. Then, figure out how to automate getting the pages of interest.\n",
    "1. Run those pages through the procedure in step 1.\n",
    "1. Clean and save.\n",
    "\n",
    "This has the benefit of solving what is usually the hardest problem first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note\n",
    "\n",
    "As you'll see, the difficulty ramps up a lot here.\n",
    "Web scraping is easily a full day topic on its own.\n",
    "Hence, I have two main goals for you:\n",
    "\n",
    "1. Get a sense of the logic and the process in solving the problem. This is a good start if you want to learn it yourself.\n",
    "1. Understand what is feasible and achievable. This helps whether you do it yourself or farm it out (and there's a ready talent pool for this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read one page\n",
    "\n",
    "This is the hardest part.\n",
    "\n",
    "Note that we add a user agent header that is sent as part of the request.\n",
    "The reason is that a lot of web servers block user agents that are web scraping tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do no change agent string, thats why its all uppercase\n",
    "# python will concat the strings in parenthese automatically\n",
    "# change agent string to correct web browser\n",
    "#\n",
    "#\n",
    "_AGENT = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0\"\n",
    "\n",
    "pr_url_1 = (\n",
    "    \"https://news.microsoft.com/2018/10/04/\"\n",
    "    \"redline-communications-and-microsoft-announce-\"\n",
    "    \"partnership-to-lower-the-cost-of-tv-white-space-solutions/\"\n",
    ")\n",
    "pr_req_1 = requests.get(pr_url_1, headers={\"User-Agent\": _AGENT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_url_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this to be 200, which is the code for OK.\n",
    "pr_req_1.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .text attribute of the request object\n",
    "# is the HTML of the page.\n",
    "pr_soup_1 = BeautifulSoup(pr_req_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The meta tags have some data we'd like to get.\n",
    "# For example, this is the published time.\n",
    "# turn on developer tools\n",
    "# tags are between lees than and greather than signs\n",
    "# standadiezed on utc time stamp\n",
    "# \"what type of tag am i trying to find\"\n",
    "# \"what are the properties im looking for\"\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the property attribute of this meta tag,\n",
    "# which has the name of the data item.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")[\"property\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The content attribute has the data item itself.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of meta tags to get.\n",
    "# Note: when in doubt, get everything you might possibly use.\n",
    "#       It's easier to drop stuff than to re-scrape everything.\n",
    "\n",
    "_METAS = [\n",
    "    \"article:published_time\",\n",
    "    \"article:modified_time\",\n",
    "    \"og:title\",\n",
    "    \"og:description\",\n",
    "    \"og:updated_time\",\n",
    "    \"og:url\",\n",
    "    \"article:section\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop populates a dict with each of the\n",
    "# meta attributes above and its content.\n",
    "# Discussion: why is this TRY/EXCEPT necessary? What happens if we remove it?\n",
    "# try runs code in the block, but if theres an error it does something\n",
    "# type error will return \"type none doesnt have a property\"\n",
    "# if that happens, dont stop running, just make the property\n",
    "#   the same as the tag and content is empty\n",
    "#  building dictionary with the stuff we want\n",
    "pr_data_1 = {}\n",
    "for meta in _METAS:\n",
    "    try:\n",
    "        prop = pr_soup_1.find(\"meta\", property=meta)[\"property\"]\n",
    "        content = pr_soup_1.find(\"meta\", property=meta)[\"content\"]\n",
    "    except TypeError:\n",
    "        prop = meta\n",
    "        content = \"\"\n",
    "    pr_data_1.update({prop: content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERY EASY TO GENERALIZE ABOVE CODES\n",
    "pr_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN USE INSPECT ELEMENT ON WEB PAGES\n",
    "# \".text\" strips html\n",
    "pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"}).find(\"h3\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"}).find(\"h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indented bc code style tool thinks the line is too long\n",
    "pr_data_1[\"h3\"] = (\n",
    "    pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"}).find(\"h3\").text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .find_all rather than just .find to get all paragraph tags\n",
    "pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"}).find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little gnarly.\n",
    "# brackets=list\n",
    "# makes a long string with all paragraphs in it\n",
    "# \"new line is \\n\\n\"\n",
    "# .join method will join everything in a list\n",
    "# displayed on a dashboard would give page breaks\n",
    "pr_data_1[\"body\"] = \"\\n\\n\".join(\n",
    "    [\n",
    "        i.text\n",
    "        for i in pr_soup_1.find(\n",
    "            \"div\", {\"class\": \"entry-content m-blog-content\"}\n",
    "        ).find_all(\"p\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate our one page work.\n",
    "\n",
    "This is fairly easy. We have the code for it already.\n",
    "We just need to wrap it in a function.\n",
    "\n",
    "**Note:** I'm using an `if` statement to check whether these properties exist, and guarding against the case where they don't.\n",
    "I did this iteratively while building this content, because I noticed (from errors) that many press releases do not have modification dates or article sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atypically long function for him. normally in pieces\n",
    "# always specify EXCEPT statement otherwise\n",
    "#    keyboard interrupts wont stop long running errors\n",
    "def get_data_from_soup(soup):\n",
    "    data = {}\n",
    "    for meta in _METAS:\n",
    "        if soup.find(\"meta\", property=meta) is not None:\n",
    "            prop = soup.find(\"meta\", property=meta)[\"property\"]\n",
    "        if soup.find(\"meta\", property=meta) is not None:\n",
    "            content = soup.find(\"meta\", property=meta)[\"content\"]\n",
    "        if prop is not None and content is not None:\n",
    "            data.update({prop: content})\n",
    "    try:\n",
    "        data[\"h3\"] = (\n",
    "            soup.find(\"div\", {\"class\": \"entry-content m-blog-content\"})\n",
    "            .find(\"h3\")\n",
    "            .string\n",
    "        )\n",
    "    except AttributeError:\n",
    "        data[\"h3\"] = \"\"\n",
    "\n",
    "    data[\"body\"] = \"\\n\\n\".join(\n",
    "        [\n",
    "            i.text\n",
    "            for i in soup.find(\n",
    "                \"div\", {\"class\": \"entry-content m-blog-content\"}\n",
    "            ).find_all(\"p\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how easy this is once we make a function.\n",
    "# make descriptive function names\n",
    "get_data_from_soup(pr_soup_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read many pages\n",
    "\n",
    "Now we need to get the URLs for all of the pages we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_pr_url_1 = \"https://news.microsoft.com/category/press-releases/\"\n",
    "many_pr_page_1 = requests.get(many_pr_url_1, headers={\"User-Agent\": _AGENT}).text\n",
    "many_pr_soup_1 = BeautifulSoup(many_pr_page_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost, but note the ones at the bottom.\n",
    "# a is an anchor tag to link in html\n",
    "many_pr_soup_1.find(\"section\", id=\"primary\").find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we further filter down to articles and then get their hrefs to\n",
    "#    eliminate the navigation links at the bottom.\n",
    "# href is hypertext reference = link\n",
    "articles = many_pr_soup_1.find(\"section\", id=\"primary\").find_all(\"article\")\n",
    "links = [i.find(\"a\")[\"href\"] for i in articles]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_pr_links_1 = links.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_pr_links_1 = links.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate getting links and data from each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to turn links into soup objects a lot, so let's make a function.\n",
    "# short functions that do one thing\n",
    "# create a list oflinks and pull data from each link\n",
    "def link_to_soup(link):\n",
    "    page = requests.get(link, headers={\"User-Agent\": _AGENT}).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_links_from_link_page(link_page):\n",
    "    soup = link_to_soup(link_page)\n",
    "    articles = soup.find(\"section\", id=\"primary\").find_all(\"article\")\n",
    "    links = [i.find(\"a\")[\"href\"] for i in articles]\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_data_from_links(links):\n",
    "    data_list = []\n",
    "    for link in links:\n",
    "        soup = link_to_soup(link)\n",
    "        data_list.append(get_data_from_soup(soup))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_prs = pd.DataFrame(get_data_from_links(many_pr_links_1))\n",
    "msft_prs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further automation\n",
    "\n",
    "**Note**: for running time reasons, we're not going to make a multi-links-page version, but note that there's a next page link at the bottom of those pages that can be extracted to build that:\n",
    "\n",
    "```html\n",
    "<a href=\"/category/press-releases/page/2/?paged=3\" \n",
    "   class=\"c-glyph x-hidden-focus\" \n",
    "   aria-label=\"Go to next page\" ms.title=\"Next Page\">\n",
    "```\n",
    "\n",
    "However, we could also notice that the link pages have a number in the URL that is incremented by one for each page.\n",
    "We would have to look at a page to get the end number, but we could also simply use a loop to construct a URL for each of those numbers.\n",
    "\n",
    "`https://news.microsoft.com/category/press-releases/page/2/`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use time .sleep to wait between each link pull"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
